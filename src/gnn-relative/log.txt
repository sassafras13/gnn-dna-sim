2023-08-26: 
- currently writing up code to produce a relative edge_attr that doesn't just contain 1s
- use the COO representation of the adjacency matrix to get the E matrix
- just gotta remember what the COO matrix representation is

2023-08-27:
- wrote a README in the dataset-generation/ subdirectory on how to generate new trajectories for training
- generated 20 additional trajectories -- at 30 total now
- updated the gnn relative to work, just need to re-write the rollout function
- ran on the absolute gnn -- try reducing the number of validation trajectories relative to training trajectories (currently 24 -- 5)

2023-09-02:
- fixed the rollout function for gnn relative
- going to try running the absolute gnn again. I reviewed the loss curve and it actually looks like the model just overfit, so I'm going to reduce the latent dimension from 128 to 64 and see what I get. 
- did that and got slightly better results in terms of the shape of the loss curve
- going to run the same parameters with my relative GNN overnight -- tomorrow, do the MLP

2023-09-03:
- realized that the output, y, was not being normalized so I made adjustments in the GNN relative code and re-ran
- ran all 3 models training on 100 epochs

2023-09-04: 
- ran analyses
- since I have more time, I want to try a couple things: 
-- try RGNN with latent dim 96 for 50 epochs -- does it perform better than latent dim 64? 
-- try RGNN with latent dim {96 or 64} and noise 0.003 for 50 epochs
- **RE-RUN WITH DIM 96 AND NOISE 0.0003 - i forgot to save!!

2023-09-05:
- re-ran latent dim 96, noise 0.0003 for 50 epochs - DONE
- then try dim 64 with noise 0.003 - DONE
- then try dim 64 with noise 0.00003 - DONE
- then try dim 64 with noise 0.0003, k = 5 - DONE
- then try dim 64 with noise 0.0003, k = 7
