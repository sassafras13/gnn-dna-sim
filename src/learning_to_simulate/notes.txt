============= 2021-11-27
- currently trying to install Tensorflow 1.15 using miniconda
- to run Tensorflow 1.15.3 I need python 3.5 so I am creating a conda
  environment py35 that has Python3.5 and Tensorflow 1.15.3
- will continue to run setup for the rest of the requirements.txt before
  trying to train the actual model
- can save requirements packages using pip freeze > name.txt
- downloaded WaterDropSample data -- don't download the others, require GB of
  space!

================ 2021-11-28
- I realized that I really need dm-tree, dm-sonnet adn graph-nets but I am
  erroring out. I think it is either because I need Python 3.6 or I need to
install Bazel.
- I installed Bazel by installing npm and then $npm install -g @bazel/bazelisk
- Install items in a requirements file via $pip install -r requirements.txt
- To create a conda environment for a particular python version: $conda create --name py33 python=3.3
- To create a requirements file: $pip freeze > requirements.txt
- Recreated a conda environment in Python 3.6 in lab computer and running
  training on that machine
- So I know that I have the code up and running, now the question is: How does
  the model want me to present the data so that it can learn from it? 
-- to answer this I need to look at my WaterDropSample dataset and see how to
recreate that data format with my information
- there is data contained in the tfrecord and in the metadata
-- in tfrecords there is particle type for key = 0 and key = 1
-- the particle type is either 000 or 005
- in the metadata:
-- bounding box dimensions
-- sequence length
-- default connectivity radius
-- dimensions (2 in this case - i.e. x and y dimensions?)
-- dt (time step)
-- mean and std dev of velocity
-- mean and std dev of acceleration

- let's go through and see if we can figure out what these mean from the
  paper?
- I think there are 2 trajectories for WaterDropSample so the key = 0, 1
  refers to which trajectory we are rolling out.
- so what do the numbers 000 and 005 mean?
- look at the goop dataset, which include friction, that might better match my
  own data
- sequence length is the number of time steps in the trajectory
- there are 9 particle types available
- the thing I am not getting is how does the model get the ground truth? the
  dataset only seems to contain the starting point position of the particles?
- the difference between the 2 rollouts I have is the starting
  configuration...

================== 2021-11-29
- the rollout data includes metadata, initial positions, particle types,
  ground truth rollout, predicted rollout
- I think the simulator model is doing both ground truth computation and
  predictions?
- okay, yeah, somehow the simulator teaches itself to predict the
  trajectories. --> NO the datasets contain the positions for every time step
its just that I was printing it wrong. The data type is
tf.train.SequenceExample
(https://www.tensorflow.org/api_docs/python/tf/train/SequenceExample)
- see this great bit of code for creating our own datasets: https://github.com/deepmind/deepmind-research/issues/199

- you can choose:
-- the particle type from their set of particle types (I suppose? I cannot
figure out what the particle numbers map to or when the properties are
learned?)--> maybe use GOOP? (7)
-- the number of particles
-- the positions of the particles over every time step

- so tomorrow's mission: use the code provided in the link above to create my
  own dataset using the data that I have
- feed this dataset to the model
- run and see if it works when representing the particles as Goop
  particles...?

======================== 2021-11-30
- consider breaking the data for cuboid into 70% train and 30% train by masking some particles during training
- including the vector of each base is important information, but will my model accept it? 
-- Chris suggested that I add a particle for the base, top and side of each nucleotide...consider it...

- currently got a baseline save_data.py to work but getting the following error when I try to read the data: 
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __inference_Dataset_map_<class 'functools.partial'>_40}} 
Input to reshape is a tensor with 8164800 values, but the requested shape requires a multiple of 303

- fixed it! just need to set sequence length in metadata to 99, not 100.

- look at datatypes when saving the data to tfrecord - they are not correct when I read the data back.
- also check the nan values in "position"
- and particle type is wrong, it's not reading the string correctly

- okay, everything works and I was able to train and plot results successfully.
- next steps:
--- train for ~100 epochs and plot results on same dataset
--- convert other .dat files to .tfrecords and use as test and validation sets 
--- try taking ~30% of the cuboid's particles and save as a test/validation set
--- try Chris' idea of adding extra particles for directionality

2021-12-01
- so it turns out that the /tmp/ folder gets purged if you shut down your computer..who knew? 
- I have created a permanent data directory in ~/gnn_dna/datasets/
- I have downloaded the llama, scotty and cuboid files. so far I've just converted the cuboid to be the train.tfrecord file and re-created the metadata file too 
- note that the metadata will change for the other structures!
- I've copied the train.tfrecord and metadata to the lab computer and I am running 100 epochs of training on that dataset
- to use imagemagick to split up a gif, do: $convert foo.gif foo.png

- results didn't look any better. Try expanding the radius of connectivity and see what other hyperparameters to try
- Chris suggested cosine curve decay for loss function, and trying 1e-3 as starting point 
- get a validation set 
- add 3x neighbors

2021-12-04
- I split the data into a train, val and test subset
- I have updated the train.py script to save a checkpoint at every step 
- I also changed train.py script to run eval on every checkpoint so now I can get train-validation curves
- Next I need to run 
